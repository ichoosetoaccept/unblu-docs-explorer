# Project Plan: Unblu Documentation MCP Server with Test-Driven Development

## Project Overview
This project creates an MCP server that makes Unblu's documentation accessible, searchable, and queryable through Claude. By following Test-Driven Development principles, we ensure each component is thoroughly tested and reliable before building upon it. This approach is particularly valuable as each phase depends on the successful implementation of previous phases.

## Phase 1: Foundation Setup

### Core Infrastructure
First, we establish our testing infrastructure alongside the base project structure:
```python
unblu_docs_explorer/
├── pyproject.toml          # Project metadata and dependencies
├── config/
│   └── settings.json      # Configuration file
├── src/
│   └── unblu_docs_explorer/
│       ├── __init__.py
│       ├── server.py      # Main MCP server implementation
│       ├── fetcher.py     # Documentation fetching logic
│       ├── processor.py   # Content processing and structuring
│       ├── cache.py       # Caching mechanisms
│       ├── errors.py      # Error handling system
│       ├── models/        # Data models and schemas
│       │   ├── __init__.py
│       │   └── document.py
│       └── utils/         # Shared utilities
│           ├── __init__.py
│           └── error_context.py
└── tests/                 # Comprehensive test suite
    ├── conftest.py       # Pytest configuration and fixtures
    ├── test_errors.py    # Error handling tests
    ├── test_fetcher.py   # Documentation fetching tests
    ├── test_processor.py # Content processing tests
    └── test_server.py    # MCP server integration tests
```

Dependencies in pyproject.toml, with comprehensive testing support:
```toml
[project]
name = "unblu-docs-mcp"
version = "0.1.0"
description = "MCP server for Unblu documentation"
requires-python = ">=3.12"

dependencies = [
    "mcp>=1.1.2",
    "httpx>=0.24.0",
    "beautifulsoup4>=4.12.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.23.0",
    "pytest-cov>=4.1.0",     # Coverage reporting
    "pytest-mock>=3.10.0",   # Mocking support
    "ruff>=0.8.3",
    "responses>=0.23.0",     # HTTP mocking
]
```

### Foundation Tests
We begin with tests that define our basic requirements:

```python
# tests/test_errors.py
import pytest
from unblu_docs_explorer.errors import ErrorContext, DocumentationError

async def test_error_context_handles_exceptions():
    """Verify error context properly transforms exceptions."""
    with pytest.raises(DocumentationError) as exc_info:
        async with ErrorContext("test_operation").handle():
            raise ValueError("Test error")
    
    assert exc_info.value.operation == "test_operation"
    assert isinstance(exc_info.value.original_error, ValueError)

# tests/conftest.py
import pytest
from pathlib import Path

@pytest.fixture
def sample_doc_html():
    """Provide sample HTML content for testing."""
    return """
    <html>
        <head><title>Test Doc</title></head>
        <body>
            <h1>Test Section</h1>
            <p>Test content</p>
        </body>
    </html>
    """

@pytest.fixture
def mock_http_client(responses):
    """Configure mock HTTP responses for testing."""
    responses.add(
        responses.GET,
        "https://docs.unblu.com/test",
        body=sample_doc_html(),
        status=200
    )
```

## Phase 2: Error Handling and Resource Management

### Error Handling Tests
Before implementing error handling, we define its expected behavior:

```python
# tests/test_errors.py
async def test_error_context_preserves_stack_trace():
    """Ensure error transformation maintains debugging information."""
    with pytest.raises(DocumentationError) as exc_info:
        async with ErrorContext("nested_operation").handle():
            try:
                raise ValueError("Inner error")
            except ValueError as e:
                raise RuntimeError("Outer error") from e
    
    assert exc_info.value.operation == "nested_operation"
    assert isinstance(exc_info.value.__cause__, ValueError)

async def test_resource_tracker_validates_resources():
    """Verify resource tracker properly manages resource lifecycle."""
    tracker = ResourceTracker()
    
    # Test resource registration
    await tracker.register("test_id", {"data": "test"})
    resource = await tracker.get("test_id")
    assert resource["data"] == "test"
    
    # Test missing resource handling
    with pytest.raises(ResourceNotFoundError):
        await tracker.get("nonexistent")
```

## Phase 3: Documentation Management System

### Documentation Fetcher Tests
Tests that drive the implementation of our documentation fetching system:

```python
# tests/test_fetcher.py
async def test_fetch_section_returns_structured_content(
    mock_http_client,
    sample_doc_html
):
    """
    Verify that fetched content is properly structured and processed.
    """
    manager = DocumentationManager("https://docs.unblu.com")
    content = await manager.fetch_section("/test")
    
    assert content["title"] == "Test Section"
    assert "Test content" in content["content"]
    assert isinstance(content["subsections"], list)

async def test_fetch_section_uses_cache(mock_http_client):
    """Verify caching behavior works as expected."""
    manager = DocumentationManager("https://docs.unblu.com")
    
    # First fetch should hit the network
    content1 = await manager.fetch_section("/test")
    
    # Second fetch should use cache
    content2 = await manager.fetch_section("/test")
    
    assert content1 == content2
    assert mock_http_client.call_count == 1
```

## Phase 4: Search Implementation

### Search System Tests
Tests that define our search functionality requirements:

```python
# tests/test_search.py
async def test_search_finds_relevant_content(
    sample_documentation
):
    """Verify search returns relevant results in correct order."""
    search = DocumentationSearch()
    results = await search.search("test query")
    
    assert len(results) > 0
    assert results[0]["relevance"] >= results[1]["relevance"]
    assert "test" in results[0]["content"].lower()

async def test_search_respects_context():
    """Verify context-aware search narrows results appropriately."""
    search = DocumentationSearch()
    results = await search.search(
        "test", 
        context="specific_section"
    )
    
    assert all(
        "specific_section" in result["path"] 
        for result in results
    )
```

## Phase 5: MCP Server Integration

### Server Integration Tests
Tests that verify the complete system works together:

```python
# tests/test_server.py
async def test_server_lists_resources():
    """Verify server properly exposes documentation sections."""
    server = UnbluDocsServer(config_path)
    resources = await server.list_resources()
    
    assert len(resources) > 0
    assert all(
        isinstance(resource, types.Resource)
        for resource in resources
    )

async def test_search_tool_integration():
    """Verify search tool functions correctly through MCP interface."""
    server = UnbluDocsServer(config_path)
    result = await server.handle_tool_call(
        "search_docs",
        {"query": "test query"}
    )
    
    assert isinstance(result, list)
    assert all(
        isinstance(item["relevance"], float)
        for item in result
    )
```

## Implementation Sequence with TDD

The implementation follows these test-driven phases:

1. Foundation Setup
   - Write basic infrastructure tests
   - Implement minimal passing code
   - Refactor for clarity and maintainability
   - Verify test coverage

2. Error and Resource Management
   - Write error handling tests
   - Implement error context system
   - Write resource tracking tests
   - Implement resource management
   - Verify error propagation and handling

3. Documentation Management
   - Write document fetching tests
   - Implement fetching system
   - Write processing tests
   - Implement content processing
   - Verify caching behavior

4. Search System
   - Write search functionality tests
   - Implement basic search
   - Write relevance scoring tests
   - Implement advanced search features
   - Verify search accuracy

5. MCP Integration
   - Write integration tests
   - Implement server interfaces
   - Write tool interaction tests
   - Implement tool handlers
   - Verify complete system functionality

Each phase follows the TDD cycle:
1. Write failing tests that define the desired behavior
2. Implement minimal code to make tests pass
3. Refactor while keeping tests green
4. Verify test coverage before moving to the next phase

This ensures each component is reliable before being used as a foundation for the next phase.